{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5c736d-a15e-4c1c-b9b2-f4365f225aca",
   "metadata": {},
   "source": [
    "# **This is the first gymnasium stable-baselines3 simple**\n",
    "\n",
    "The provided code implements a simple custom OpenAI Gymnasium environment for a \"Golf\" game. Here's a summary of the functionality:\n",
    "Steps:\n",
    "- Define custom env\n",
    "- Define model\n",
    "- Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09badcab-1b12-4c0b-af0d-6fc182590d4a",
   "metadata": {},
   "source": [
    "### **Key Features**\n",
    "1. **Environment Basics**:\n",
    "   - The environment models a \"golf ball\" that can move between positions on a one-dimensional grid with values ranging from `0` to `10`.\n",
    "   - The starting position of the ball is `9`.\n",
    "\n",
    "2. **Spaces**:\n",
    "   - **Action Space**: Discrete with two possible actions:\n",
    "     - `0`: Move the ball one step closer to `0` (decrement position).\n",
    "     - `1`: Move the ball one step away from `0` (increment position).\n",
    "   - **Observation Space**: A single integer (wrapped in a NumPy array) representing the ball's position, constrained between `0` and `10`.\n",
    "\n",
    "3. **Core Methods**:\n",
    "   - **`reset()`**: \n",
    "     - Resets the environment, placing the ball at position `9`.\n",
    "     - Returns the initial observation and an empty dictionary.\n",
    "   - **`step(action)`**:\n",
    "     - Updates the position based on the action taken.\n",
    "     - Clips the position to ensure it remains between `0` and `10`.\n",
    "     - Checks if the game is complete (`done` is `True`) when the position reaches `0`.\n",
    "     - Rewards the agent with `1` if the game ends, otherwise `0`.\n",
    "     - Returns the updated observation, reward, done flag, truncated flag (`False` in this case), and an empty info dictionary.\n",
    "   - **`render()`**: Prints the current position of the ball to the console.\n",
    "   - **`close()`**: A placeholder method for cleanup.\n",
    "\n",
    "### **Purpose**\n",
    "This code defines a simple reinforcement learning environment where the goal is to reach position `0` from an initial position of `9`. It could be used to test or train RL agents in a straightforward setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c1d322-b825-4b7a-a51a-f6b220fc8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, SupportsFloat\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.core import ActType\n",
    "from gymnasium.core import ObsType\n",
    "\n",
    "\n",
    "class GolfEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pos = 9\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=10,\n",
    "            shape=(1,),\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "\n",
    "    def reset(\n",
    "            self,\n",
    "            *,\n",
    "            seed: int | None = None,\n",
    "            options: dict[str, Any] | None = None,\n",
    "    ) -> tuple[ObsType, dict[str, Any]]:  # type: ignore\n",
    "\n",
    "        self.pos = 9\n",
    "        return np.array([self.pos]), {}\n",
    "\n",
    "    def step(\n",
    "            self, action: ActType\n",
    "    ) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "\n",
    "        if action == 0:\n",
    "            self.pos -= 1\n",
    "\n",
    "        if action == 1:\n",
    "            self.pos += 1\n",
    "\n",
    "        self.pos = np.clip(self.pos, 0, 10)\n",
    "\n",
    "        done = bool(self.pos == 0)\n",
    "        reward = 1 if done else 0\n",
    "\n",
    "        return np.array([self.pos]), reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(f'pos: {self.pos}')\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb77a45-7796-46e9-981b-9e2d703c0506",
   "metadata": {},
   "source": [
    "### **Key Features**\n",
    "1. **Purpose**:\n",
    "   - The code defines a custom neural network for an Actor-Critic policy used in reinforcement learning. It extends the `ActorCriticPolicy` class from the `Stable-Baselines3` library and introduces a user-defined `MyACModel` for the Actor and Critic networks.\n",
    "\n",
    "2. **Classes**:\n",
    "   - **`MyACModel`**:\n",
    "     - Implements separate feedforward networks for the Actor (policy) and Critic (value function).\n",
    "     - Provides methods to compute forward passes for the Actor, Critic, or both combined.\n",
    "   - **`MyACPolicy`**:\n",
    "     - Extends the `ActorCriticPolicy` class from `Stable-Baselines3`.\n",
    "     - Overrides the `_build_mlp_extractor` method to use the custom `MyACModel` as the policy's neural network.\n",
    "\n",
    "3. **Customizations**:\n",
    "   - The Actor and Critic networks have independent architectures, each with a configurable output dimension (`last_layer_dim_pi` and `last_layer_dim_vf`).\n",
    "   - Orthogonal initialization is disabled in this policy implementation.\n",
    "\n",
    "4. **Use Case**:\n",
    "   - This implementation can be used to define a custom Actor-Critic policy for reinforcement learning algorithms, such as PPO, A2C, or other variants supported by `Stable-Baselines3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebcce0f0-40d3-439b-b545-3a9ec3650606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "\n",
    "class MyACModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        features_dim,\n",
    "        last_layer_dim_pi=64,\n",
    "        last_layer_dim_vf=64\n",
    "    ):\n",
    "        super(MyACModel, self).__init__()\n",
    "\n",
    "        # Store the output dimensions for the Actor and Critic\n",
    "        self.latent_dim_pi = last_layer_dim_pi  # Output dimension of the policy network (Actor)\n",
    "        self.latent_dim_vf = last_layer_dim_vf  # Output dimension of the value function network (Critic)\n",
    "\n",
    "        # Define the Actor network\n",
    "        self.actor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(features_dim, last_layer_dim_pi),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Define the Critic network\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            torch.nn.Linear(features_dim, last_layer_dim_vf),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for both Actor and Critic networks, used during testing.\n",
    "        \"\"\"\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        return actor_output, critic_output\n",
    "\n",
    "    def forward_actor(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the Actor network, used for policy prediction.\n",
    "        \"\"\"\n",
    "        return self.actor(x)\n",
    "\n",
    "    def forward_critic(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the Critic network, used for value function computation.\n",
    "        \"\"\"\n",
    "        return self.critic(x)\n",
    "\n",
    "\n",
    "class MyACPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        lr_schedule,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(obs_space, action_space, lr_schedule, *args, **kwargs)\n",
    "        self.ortho_init = False  # Disable orthogonal initialization (can be enabled based on requirements)\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        # Initialize the custom MLP extractor\n",
    "        self.mlp_extractor = MyACModel(self.features_dim)\n",
    "        # Set the output dimensions for the Actor and Critic\n",
    "        self.latent_dim_pi = self.mlp_extractor.latent_dim_pi\n",
    "        self.latent_dim_vf = self.mlp_extractor.latent_dim_vf\n",
    "\n",
    "    def _train(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab42aaa5-42d0-4f57-9546-9fc87ca82672",
   "metadata": {},
   "source": [
    "### **Key Components**\n",
    "\n",
    "#### **1. Training Function (`train`)**\n",
    "- **Inputs**: \n",
    "  - `NewEnv`: The environment class to be trained on.\n",
    "- **Process**:\n",
    "  - Creates a vectorized environment using `make_vec_env`, allowing multiple instances (10 in this case) of the environment to be run in parallel for faster training.\n",
    "  - Initializes a PPO model with:\n",
    "    - `MyACPolicy`: The custom policy defined in `MyACPolicy`.\n",
    "    - `train_env`: The vectorized training environment.\n",
    "  - Trains the model for `20,000` timesteps.\n",
    "- **Output**: The trained PPO model.\n",
    "\n",
    "#### **2. Testing Function (`test`)**\n",
    "- **Inputs**:\n",
    "  - `model`: The trained RL model.\n",
    "  - `env`: A single instance of the environment for testing.\n",
    "- **Process**:\n",
    "  - Resets the environment to obtain the initial observation.\n",
    "  - Runs a loop for up to `100` steps:\n",
    "    - Uses the trained model to predict actions based on the current observation.\n",
    "    - Executes the action in the environment and receives:\n",
    "      - The next observation.\n",
    "      - Reward for the action.\n",
    "      - Done flag (indicating the end of the episode).\n",
    "    - Prints the current state (`obs`), action taken, reward received, and whether the episode is done.\n",
    "    - Breaks out of the loop if the episode is complete (`done` is `True`).\n",
    "- **Output**: None. Results are printed to the console.\n",
    "\n",
    "---\n",
    "\n",
    "### **Execution Flow (`__main__`)**\n",
    "1. **Training**:\n",
    "   - Calls `train(GolfEnv)` to train the PPO model using the custom `GolfEnv` environment.\n",
    "2. **Testing**:\n",
    "   - Calls `test(model, GolfEnv())` to evaluate the trained model on a single instance of `GolfEnv`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Use Case**\n",
    "- This script demonstrates a full RL pipeline:\n",
    "  - **Environment Setup**: Defines and uses a custom Gymnasium environment (`GolfEnv`).\n",
    "  - **Policy Customization**: Implements a custom Actor-Critic policy (`MyACPolicy`).\n",
    "  - **Training**: Trains the policy using PPO.\n",
    "  - **Testing**: Evaluates the trained policy in the environment, displaying key results like actions, rewards, and terminal conditions.\n",
    "\n",
    "This setup is ideal for experimenting with custom RL environments and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6825f3c7-d35c-400e-aace-bd296a189b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 12051    |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "obs: [8], action: 0, reward: 0, done: False\n",
      "obs: [7], action: 0, reward: 0, done: False\n",
      "obs: [8], action: 1, reward: 0, done: False\n",
      "obs: [7], action: 0, reward: 0, done: False\n",
      "obs: [6], action: 0, reward: 0, done: False\n",
      "obs: [7], action: 1, reward: 0, done: False\n",
      "obs: [8], action: 1, reward: 0, done: False\n",
      "obs: [7], action: 0, reward: 0, done: False\n",
      "obs: [6], action: 0, reward: 0, done: False\n",
      "obs: [7], action: 1, reward: 0, done: False\n",
      "obs: [8], action: 1, reward: 0, done: False\n",
      "obs: [9], action: 1, reward: 0, done: False\n",
      "obs: [8], action: 0, reward: 0, done: False\n",
      "obs: [7], action: 0, reward: 0, done: False\n",
      "obs: [8], action: 1, reward: 0, done: False\n",
      "obs: [9], action: 1, reward: 0, done: False\n",
      "obs: [10], action: 1, reward: 0, done: False\n",
      "obs: [10], action: 1, reward: 0, done: False\n",
      "obs: [9], action: 0, reward: 0, done: False\n",
      "obs: [8], action: 0, reward: 0, done: False\n",
      "obs: [7], action: 0, reward: 0, done: False\n",
      "obs: [8], action: 1, reward: 0, done: False\n",
      "obs: [7], action: 0, reward: 0, done: False\n",
      "obs: [8], action: 1, reward: 0, done: False\n",
      "obs: [7], action: 0, reward: 0, done: False\n",
      "obs: [8], action: 1, reward: 0, done: False\n",
      "obs: [7], action: 0, reward: 0, done: False\n",
      "obs: [8], action: 1, reward: 0, done: False\n",
      "obs: [9], action: 1, reward: 0, done: False\n",
      "obs: [8], action: 0, reward: 0, done: False\n",
      "obs: [7], action: 0, reward: 0, done: False\n",
      "obs: [8], action: 1, reward: 0, done: False\n",
      "obs: [9], action: 1, reward: 0, done: False\n",
      "obs: [10], action: 1, reward: 0, done: False\n",
      "obs: [10], action: 1, reward: 0, done: False\n",
      "obs: [10], action: 1, reward: 0, done: False\n",
      "obs: [9], action: 0, reward: 0, done: False\n",
      "obs: [10], action: 1, reward: 0, done: False\n",
      "obs: [10], action: 1, reward: 0, done: False\n",
      "obs: [9], action: 0, reward: 0, done: False\n",
      "obs: [8], action: 0, reward: 0, done: False\n",
      "obs: [7], action: 0, reward: 0, done: False\n",
      "obs: [8], action: 1, reward: 0, done: False\n",
      "obs: [9], action: 1, reward: 0, done: False\n",
      "obs: [10], action: 1, reward: 0, done: False\n",
      "obs: [9], action: 0, reward: 0, done: False\n",
      "obs: [10], action: 1, reward: 0, done: False\n",
      "obs: [9], action: 0, reward: 0, done: False\n",
      "obs: [8], action: 0, reward: 0, done: False\n",
      "obs: [7], action: 0, reward: 0, done: False\n",
      "obs: [6], action: 0, reward: 0, done: False\n",
      "obs: [5], action: 0, reward: 0, done: False\n",
      "obs: [4], action: 0, reward: 0, done: False\n",
      "obs: [3], action: 0, reward: 0, done: False\n",
      "obs: [2], action: 0, reward: 0, done: False\n",
      "obs: [3], action: 1, reward: 0, done: False\n",
      "obs: [4], action: 1, reward: 0, done: False\n",
      "obs: [3], action: 0, reward: 0, done: False\n",
      "obs: [2], action: 0, reward: 0, done: False\n",
      "obs: [1], action: 0, reward: 0, done: False\n",
      "obs: [0], action: 0, reward: 1, done: True\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "\n",
    "def train(NewEnv):\n",
    "    train_env = make_vec_env(lambda: NewEnv(), n_envs=10)\n",
    "    model = PPO(MyACPolicy, env=train_env, verbose=0)\n",
    "    model.learn(total_timesteps=2_0000)\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(model, env):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    for i in range(100):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        print(f'obs: {obs}, action: {action}, reward: {reward}, done: {done}')\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "model = train(GolfEnv)\n",
    "test(model, GolfEnv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a97507-14c9-499b-9345-e77fc6880a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
