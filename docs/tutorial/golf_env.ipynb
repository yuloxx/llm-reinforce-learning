{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c598c4ba-eec1-4f0a-9301-157e8ef00d7f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# **More complicated gymnasium environment**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee310f-3f80-4bc7-9696-074c8f6bbfa1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **1. Preparation**\n",
    "Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0cc9dec-845d-4ee5-80f9-2187174021c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, SupportsFloat, Dict\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.core import ActType, ObsType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ba8a48-3a97-47d9-86eb-867b0c2f39ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_vec_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49334a29-a0c1-4d20-ab40-1611d939d85b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **2. Golf2DEnv: A Simple 2D Golf Game Environment**\n",
    "### Overview\n",
    "`Golf2DEnv` is a simple 2D reinforcement learning environment implemented using OpenAI Gym. The goal of the game is to move a point (the \"ball\") to the target position `[5,5]` on a 10x10 grid. The environment provides a discrete action space for movement and assigns rewards based on the Manhattan distance from the target.\n",
    "\n",
    "### Environment Description\n",
    "- The environment is represented as a 10x10 grid.\n",
    "- The starting position is `[0,0]`.\n",
    "- The target position is `[5,5]`.\n",
    "- The observation space is a 2D integer coordinate within the range `[0,10]`.\n",
    "- The environment supports only console-based rendering.\n",
    "\n",
    "### Game Objective\n",
    "The agent needs to reach the goal position `[5,5]` from any given starting position. The reward function is based on the negative Manhattan distance from the target, encouraging the agent to minimize its distance to the goal.\n",
    "\n",
    "### Actions\n",
    "The environment provides a discrete action space with four possible movements:\n",
    "\n",
    "| Action | Description |\n",
    "|--------|-------------|\n",
    "| 0 | Move left (decrease x-coordinate) |\n",
    "| 1 | Move up (increase y-coordinate) |\n",
    "| 2 | Move right (increase x-coordinate) |\n",
    "| 3 | Move down (decrease y-coordinate) |\n",
    "\n",
    "### Step Method Breakdown\n",
    "Each step in the environment follows these operations:\n",
    "1. The agent takes an action from the discrete action space.\n",
    "2. The agent's position updates based on the chosen action.\n",
    "3. The position is clipped within the 10x10 grid.\n",
    "4. The reward is computed as the negative Manhattan distance from the target `[5,5]`.\n",
    "5. The environment checks if the agent has reached the goal (`done` state).\n",
    "6. The method returns the new state, reward, done flag, and additional information.\n",
    "\n",
    "### Expected Results\n",
    "- The agent receives negative rewards until it reaches `[5,5]`.\n",
    "- The optimal strategy minimizes the number of steps to the goal.\n",
    "- Upon reaching `[5,5]`, the episode terminates.\n",
    "- The environment provides a simple testing ground for reinforcement learning algorithms with discrete action spaces and reward shaping based on distance minimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e21345d-be0b-4808-94c1-b2161bc73641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Golf2DEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos = np.array([0, 0], dtype=np.int32)\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        # 2D Box\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=10,\n",
    "            shape=(2,),\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "\n",
    "    def reset(\n",
    "            self,\n",
    "            *,\n",
    "            seed: int | None = None,\n",
    "            options: dict[str, Any] | None = None,\n",
    "    ) -> tuple[ObsType, dict[str, Any]]:  # type: ignore\n",
    "        self.pos = np.array([0, 0], dtype=np.int32)\n",
    "        # self.pos = self.np_random.integers(low=0, high=10, size=(2,))\n",
    "        return self.pos, {}\n",
    "\n",
    "    def _get_reward(self) -> int:\n",
    "        return -abs(self.pos[0] - 5) - abs(self.pos[1] - 5)\n",
    "\n",
    "    def _is_done(self) -> bool:\n",
    "        return np.array_equal(self.pos, np.array([5, 5]))\n",
    "\n",
    "    def step(\n",
    "            self, action: ActType\n",
    "    ) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "\n",
    "        if action == 0:\n",
    "            self.pos[0] -= 1\n",
    "        elif action == 1:\n",
    "            self.pos[1] += 1\n",
    "        elif action == 2:\n",
    "            self.pos[0] += 1\n",
    "        elif action == 3:\n",
    "            self.pos[1] -= 1\n",
    "\n",
    "        self.pos = np.clip(self.pos, 0, 10)\n",
    "\n",
    "        reward = self._get_reward()\n",
    "\n",
    "        done = self._is_done()\n",
    "\n",
    "        return self.pos, reward, done, False, {}\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        if mode == 'console':\n",
    "            print(f\"Current Position: {self.pos}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e3139-9066-4e36-bcab-71e28101aa42",
   "metadata": {},
   "source": [
    "### Train And Test\n",
    "\n",
    "- **Algorithm**  \n",
    "  - The code uses **PPO (Proximal Policy Optimization)**, a reinforcement learning algorithm.  \n",
    "  - PPO optimizes the policy by limiting the step size of updates, ensuring stable learning.  \n",
    "  - It is widely used for continuous control tasks due to its balance between sample efficiency and stability.  \n",
    "\n",
    "- **Process**  \n",
    "  - **Training:**  \n",
    "    - A vectorized environment (`make_vec_env`) is created with 10 parallel instances.  \n",
    "    - A PPO model is initialized with the `MlpPolicy` (Multi-Layer Perceptron policy).  \n",
    "    - The model is trained for `100000` timesteps.  \n",
    "\n",
    "  - **Testing:**  \n",
    "    - The trained model is tested in a single instance of the environment.  \n",
    "    - The environment is reset before testing.  \n",
    "    - The model predicts actions step by step for up to 100 iterations or until `done`.  \n",
    "    - Observations, actions, rewards, and termination signals are printed.  \n",
    "\n",
    "- **Validation**  \n",
    "  - The test phase evaluates whether the trained policy performs well in the environment.  \n",
    "  - Observing the reward values and termination conditions helps determine policy effectiveness.  \n",
    "  - If performance is suboptimal, hyperparameters (e.g., learning rate, network architecture) can be adjusted to improve learning.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbe1e709-d68b-41d4-a7e1-948ef4aaacbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: [0 1], action: 1, reward: -9, done: False\n",
      "obs: [0 2], action: 1, reward: -8, done: False\n",
      "obs: [1 2], action: 2, reward: -7, done: False\n",
      "obs: [1 1], action: 3, reward: -8, done: False\n",
      "obs: [1 2], action: 1, reward: -7, done: False\n",
      "obs: [2 2], action: 2, reward: -6, done: False\n",
      "obs: [3 2], action: 2, reward: -5, done: False\n",
      "obs: [3 3], action: 1, reward: -4, done: False\n",
      "obs: [3 4], action: 1, reward: -3, done: False\n",
      "obs: [4 4], action: 2, reward: -2, done: False\n",
      "obs: [4 3], action: 3, reward: -3, done: False\n",
      "obs: [4 4], action: 1, reward: -2, done: False\n",
      "obs: [4 5], action: 1, reward: -1, done: False\n",
      "obs: [5 5], action: 2, reward: 0, done: True\n"
     ]
    }
   ],
   "source": [
    "def run_golf2d_env():\n",
    "    \n",
    "    def train(NewEnv):\n",
    "        train_env = make_vec_env(lambda: NewEnv(), n_envs=10)\n",
    "        model = PPO('MlpPolicy', env=train_env, verbose=0)\n",
    "        model.learn(total_timesteps=10_0000)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def test(model, env):\n",
    "        obs, info = env.reset()\n",
    "\n",
    "        for i in range(100):\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            print(f'obs: {obs}, action: {action}, reward: {reward}, done: {done}')\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "    model = train(Golf2DEnv)\n",
    "    test(model, Golf2DEnv())\n",
    "    \n",
    "run_golf2d_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e143934b-1632-4ac1-b5d0-d2d97dfe379d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **3. Golf2DBoxEnv: Multi Factor Observation In Dict Type**\n",
    "\n",
    "### Overview\n",
    "The `Golf2DBoxEnv` is a custom Gym environment designed for a simple 2D grid-based game. The environment simulates an agent navigating a 10x10 grid to collect chests. The agent's position and the positions of the chests are represented in a 2D space, and the agent can move in four cardinal directions to interact with the environment.\n",
    "\n",
    "### Environment Description\n",
    "- The environment consists of a 10x10 grid.\n",
    "- The agent starts at the position `[0, 0]`.\n",
    "- The grid contains randomly generated chests, with the number of chests ranging between 2 and 4.\n",
    "- The observation space is a dictionary containing:\n",
    "  - `agent_pos`: A 2D vector representing the agent's current position on the grid (values range from 0 to 10).\n",
    "  - `magic_box`: A 3x2 matrix representing the positions of the chests, padded with `-1` if there are fewer than 3 chests.\n",
    "\n",
    "### Game Objective\n",
    "- The goal of the game is for the agent to collect all the chests on the grid.\n",
    "- The game ends when all chests have been collected.\n",
    "\n",
    "### Action Behavior\n",
    "The action space is discrete with 4 possible actions:\n",
    "- **0**: Move left (decrease the x-coordinate).\n",
    "- **1**: Move up (increase the y-coordinate).\n",
    "- **2**: Move right (increase the x-coordinate).\n",
    "- **3**: Move down (decrease the y-coordinate).\n",
    "- The agent's position is clipped to ensure it stays within the bounds of the grid (0 to 10).\n",
    "\n",
    "### Step Method\n",
    "The `step` method allows the agent to interact with the environment. Here is the breakdown of its steps:\n",
    "1. **Action Execution**:\n",
    "   - The agent's position is updated based on the chosen action.\n",
    "2. **Position Clipping**:\n",
    "   - The agent's position is clipped to ensure it remains within the grid bounds.\n",
    "3. **Reward Calculation**:\n",
    "   - A reward of `10` is given if the agent collects a chest.\n",
    "   - The collected chest is removed from the environment.\n",
    "4. **Termination Check**:\n",
    "   - The episode ends if all chests are collected.\n",
    "5. **Observation Return**:\n",
    "   - The updated observation (agent position and chest positions) is returned.\n",
    "\n",
    "### Expected Results\n",
    "- The agent will navigate the grid to collect chests.\n",
    "- The game will terminate once all chests are collected.\n",
    "- The agent will receive a reward of `10` for each chest collected.\n",
    "- The observation space will reflect the agent's current position and the remaining chests.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45b2f5e2-2ad1-40d5-bb37-aa84eb66b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Golf2DBoxEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos = np.array([0, 0], dtype=np.int32)\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        # 2D Box\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=10,\n",
    "            shape=(2,),\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "\n",
    "        self.observation_space = gym.spaces.Dict({\n",
    "            \"agent_pos\": gym.spaces.Box(low=0, high=10, shape=(2,), dtype=np.int32),\n",
    "            \"magic_box\": gym.spaces.Box(low=0, high=10, shape=(3, 2), dtype=np.int32)\n",
    "        })\n",
    "\n",
    "        self.chests = self._generate_chests()\n",
    "\n",
    "    def _generate_chests(self) -> np.ndarray:\n",
    "        num_chests = np.random.randint(2, 4)\n",
    "        chests = np.random.randint(low=0, high=11, size=(num_chests, 2))\n",
    "        return chests\n",
    "\n",
    "    def _get_obs(self) -> Dict[str, np.ndarray]:\n",
    "        padded_chests = np.full((3, 2), -1, dtype=np.int32)\n",
    "        padded_chests[:len(self.chests)] = self.chests\n",
    "        return {\n",
    "            'agent_pos': self.pos,\n",
    "            'magic_box': padded_chests,\n",
    "        }\n",
    "\n",
    "    def _get_reward(self) -> int:\n",
    "        reward = 0\n",
    "        for i, chest in enumerate(self.chests):\n",
    "            if np.array_equal(self.pos, chest):\n",
    "                reward += 10\n",
    "                self.chests = np.delete(self.chests, i, axis=0)\n",
    "                break\n",
    "        return reward\n",
    "\n",
    "    def _is_done(self) -> bool:\n",
    "        return len(self.chests) == 0\n",
    "\n",
    "    def reset(\n",
    "            self,\n",
    "            *,\n",
    "            seed: int | None = None,\n",
    "            options: dict[str, Any] | None = None,\n",
    "    ) -> tuple[ObsType, dict[str, Any]]:  # type: ignore\n",
    "\n",
    "        self.pos = np.array([0, 0], dtype=np.int32)\n",
    "        self.chests = self._generate_chests()\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(\n",
    "            self, action: ActType\n",
    "    ) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "\n",
    "        if action == 0:\n",
    "            self.pos[0] -= 1\n",
    "        elif action == 1:\n",
    "            self.pos[1] += 1\n",
    "        elif action == 2:\n",
    "            self.pos[0] += 1\n",
    "        elif action == 3:\n",
    "            self.pos[1] -= 1\n",
    "\n",
    "        self.pos = np.clip(self.pos, 0, 10)\n",
    "\n",
    "        reward = self._get_reward()\n",
    "\n",
    "        done = self._is_done()\n",
    "\n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        if mode == 'console':\n",
    "            print(f\"Agent Position: {self.pos}\")\n",
    "            print(f\"Remaining Chests: {self.chests}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf64872b-48ed-4257-8332-f65d6a02297b",
   "metadata": {},
   "source": [
    "### Train And Test  \n",
    "\n",
    "- **Algorithm**  \n",
    "  - The code uses **PPO (Proximal Policy Optimization)**, a policy gradient reinforcement learning algorithm.  \n",
    "  - PPO optimizes the policy by **limiting large updates**, ensuring stable and consistent training.  \n",
    "  - It is effective for environments with **continuous action spaces**, making it suitable for this task.  \n",
    "\n",
    "- **Process**  \n",
    "  - **Training:**  \n",
    "    - A vectorized environment (`make_vec_env`) is created with **10 parallel instances**.  \n",
    "    - The PPO model is initialized with the **MultiInputPolicy**, which processes multiple observation inputs.  \n",
    "    - The model is trained for **100,000 timesteps** to learn an optimal movement strategy.  \n",
    "\n",
    "  - **Testing:**  \n",
    "    - The trained model is evaluated in a **single instance** of `Golf2DBoxEnv`.  \n",
    "    - The environment is **reset** before testing.  \n",
    "    - The model **predicts actions** and interacts with the environment for **up to 100 steps** or until it reaches the goal.  \n",
    "    - The agent’s **observations, actions, rewards, and termination status** are printed to monitor performance.  \n",
    "\n",
    "- **Validation**  \n",
    "  - The test phase **verifies** whether the agent successfully learns to reach the target efficiently.  \n",
    "  - Observing **reward trends** and **termination conditions** helps assess learning success.  \n",
    "  - If performance is poor, **hyperparameters** such as **learning rate, training steps, or reward function** may need adjustment.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e86c7018-ed28-4771-b7e1-847a41875ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: {'agent_pos': array([0, 0]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 0, reward: 0, done: False\n",
      "obs: {'agent_pos': array([0, 1]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([0, 2]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([1, 2]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([2, 2]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([2, 1]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([2, 0]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([2, 0]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([3, 0]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([4, 0]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([4, 0]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([4, 1]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([5, 1]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([6, 1]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([7, 1]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([7, 0]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([8, 0]), 'magic_box': array([[ 8,  1],\n",
      "       [10,  8],\n",
      "       [ 3,  2]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([8, 1]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 1, reward: 10, done: False\n",
      "obs: {'agent_pos': array([8, 0]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([8, 1]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([8, 2]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([8, 3]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([8, 4]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([9, 4]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([10,  4]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([10,  5]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([9, 5]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 0, reward: 0, done: False\n",
      "obs: {'agent_pos': array([9, 6]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([9, 7]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([8, 7]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 0, reward: 0, done: False\n",
      "obs: {'agent_pos': array([8, 8]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([7, 8]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 0, reward: 0, done: False\n",
      "obs: {'agent_pos': array([8, 8]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([9, 8]), 'magic_box': array([[10,  8],\n",
      "       [ 3,  2],\n",
      "       [-1, -1]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([10,  8]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 2, reward: 10, done: False\n",
      "obs: {'agent_pos': array([10,  7]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([10,  6]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([10,  5]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([10,  6]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([9, 6]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 0, reward: 0, done: False\n",
      "obs: {'agent_pos': array([9, 5]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([9, 4]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([10,  4]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 2, reward: 0, done: False\n",
      "obs: {'agent_pos': array([10,  3]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([9, 3]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 0, reward: 0, done: False\n",
      "obs: {'agent_pos': array([9, 2]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([8, 2]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 0, reward: 0, done: False\n",
      "obs: {'agent_pos': array([8, 3]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([7, 3]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 0, reward: 0, done: False\n",
      "obs: {'agent_pos': array([7, 4]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 1, reward: 0, done: False\n",
      "obs: {'agent_pos': array([6, 4]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 0, reward: 0, done: False\n",
      "obs: {'agent_pos': array([5, 4]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 0, reward: 0, done: False\n",
      "obs: {'agent_pos': array([5, 3]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([5, 2]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([4, 2]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 0, reward: 0, done: False\n",
      "obs: {'agent_pos': array([4, 1]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 3, reward: 0, done: False\n",
      "obs: {'agent_pos': array([3, 1]), 'magic_box': array([[ 3,  2],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 0, reward: 0, done: False\n",
      "obs: {'agent_pos': array([3, 2]), 'magic_box': array([[-1, -1],\n",
      "       [-1, -1],\n",
      "       [-1, -1]])}, action: 1, reward: 10, done: True\n"
     ]
    }
   ],
   "source": [
    "def run_golf2d_box_env():\n",
    "    \n",
    "    def train(NewEnv):\n",
    "        train_env = make_vec_env(lambda: NewEnv(), n_envs=10)\n",
    "        model = PPO('MultiInputPolicy', env=train_env, verbose=0)\n",
    "        model.learn(total_timesteps=10_0000)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def test(model, env):\n",
    "        obs, info = env.reset()\n",
    "\n",
    "        for i in range(100):\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            print(f'obs: {obs}, action: {action}, reward: {reward}, done: {done}')\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "    model = train(Golf2DBoxEnv)\n",
    "    test(model, Golf2DBoxEnv())\n",
    "    \n",
    "run_golf2d_box_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a7b960-a735-4366-a2ac-45dbece4cd18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **4. Golf2DMultiDiscreteBoxEnv: Multi-Discrete Action Sample**\n",
    "\n",
    "### Overview  \n",
    "`Golf2DMultiDiscreteBoxEnv` is a **custom reinforcement learning environment** implemented using **Gym**. The environment simulates a simple **2D grid-based movement** where an agent navigates toward a target using discrete directional actions and step sizes.  \n",
    "\n",
    "### Environment Description  \n",
    "- The environment represents a **2D grid** with boundaries between `[0, 10]` for both x and y coordinates.  \n",
    "- The agent starts at position `[0, 0]` and moves toward the target at `[5, 5]`.  \n",
    "- The action space consists of a **direction and a step size**, influencing the agent's movement.  \n",
    "- The episode terminates once the agent reaches the target.  \n",
    "\n",
    "### Game Objective  \n",
    "- The agent's goal is to **reach the target position `[5, 5]`** as efficiently as possible.  \n",
    "- The reward function is based on the **negative Manhattan distance**, meaning the closer the agent is to the goal, the higher the reward.  \n",
    "- The agent should optimize its movement strategy to minimize the number of steps taken.  \n",
    "\n",
    "### Action Behavior  \n",
    "- The action space is **MultiDiscrete([4, 5])**, where:  \n",
    "  - **First dimension (Direction)**:  \n",
    "    - `0`: Left  \n",
    "    - `1`: Up  \n",
    "    - `2`: Right  \n",
    "    - `3`: Down  \n",
    "  - **Second dimension (Step Size)**:  \n",
    "    - Ranges from `1` to `5`, indicating the number of steps moved in the chosen direction.  \n",
    "\n",
    "### Step Method Process  \n",
    "1. **Unpack the action** into `direction` and `steps`.  \n",
    "2. **Convert steps** from `[0-4]` to `[1-5]` to ensure a valid movement range.  \n",
    "3. **Update the agent’s position** based on the chosen direction and step size.  \n",
    "4. **Clip the position** within the valid grid boundaries `[0, 10]`.  \n",
    "5. **Calculate the reward** based on the **negative Manhattan distance** to the goal.  \n",
    "6. **Check if the episode is done**, i.e., the agent reaches `[5, 5]`.  \n",
    "7. **Return the new state, reward, done flag, and additional information**.  \n",
    "\n",
    "### Expected Outcome  \n",
    "- The agent learns to **navigate efficiently** toward `[5, 5]` using optimal step sizes and directions.  \n",
    "- **Higher rewards** are achieved when the agent reaches the target faster.  \n",
    "- The environment provides a simple but effective testbed for **discrete action-space reinforcement learning** algorithms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988cddb-9daf-4d39-a6a3-6ac4aae527de",
   "metadata": {},
   "source": [
    "**Key Features of MultiDiscrete:**\n",
    "\n",
    "1. Vector of Discrete Actions:\n",
    "- The action is represented as a vector (or array) of integers.\n",
    "- Each element in the vector corresponds to a separate discrete action with its own range of possible values.\n",
    "\n",
    "2. Independent Ranges:\n",
    "- Each dimension of the vector can have a different range of values.\n",
    "- For example, one dimension might have 3 possible values (0, 1, 2), while another might have 5 possible values (0, 1, 2, 3, 4).\n",
    "\n",
    "3. Use Cases:\n",
    "- MultiDiscrete is commonly used in environments where multiple discrete decisions need to be made simultaneously.\n",
    "- Examples include controlling multiple agents, selecting actions for different components of a system, or making decisions in a multi-dimensional discrete space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5435fc6a-d994-487e-8281-e22f16fac2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Golf2DMultiDiscreteBoxEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial position\n",
    "        self.pos = np.array([0, 0], dtype=np.int32)\n",
    "\n",
    "        # Define the action space as MultiDiscrete\n",
    "        # First dimension: direction (0: left, 1: up, 2: right, 3: down)\n",
    "        # Second dimension: number of steps (1 to 5)\n",
    "        self.action_space = gym.spaces.MultiDiscrete([4, 5])\n",
    "\n",
    "        # Define the observation space as a 2D Box\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=10,\n",
    "            shape=(2,),\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "\n",
    "    def reset(\n",
    "            self,\n",
    "            *,\n",
    "            seed: int | None = None,\n",
    "            options: dict[str, Any] | None = None,\n",
    "    ) -> tuple[np.ndarray, dict[str, Any]]:\n",
    "        # Reset the position to [0, 0]\n",
    "        self.pos = np.array([0, 0], dtype=np.int32)\n",
    "        return self.pos, {}\n",
    "\n",
    "    def _get_reward(self) -> int:\n",
    "        # Reward is the negative Manhattan distance to the target [5, 5]\n",
    "        return -abs(self.pos[0] - 5) - abs(self.pos[1] - 5)\n",
    "\n",
    "    def _is_done(self) -> bool:\n",
    "        # Episode ends when the agent reaches the target [5, 5]\n",
    "        return np.array_equal(self.pos, np.array([5, 5]))\n",
    "\n",
    "    def step(\n",
    "            self, action: np.ndarray\n",
    "    ) -> tuple[np.ndarray, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "        # Unpack the action\n",
    "        direction, steps = action\n",
    "        steps += 1  # Convert steps from 0-4 to 1-5\n",
    "\n",
    "        # Update position based on direction and steps\n",
    "        if direction == 0:  # Left\n",
    "            self.pos[0] -= steps\n",
    "        elif direction == 1:  # Up\n",
    "            self.pos[1] += steps\n",
    "        elif direction == 2:  # Right\n",
    "            self.pos[0] += steps\n",
    "        elif direction == 3:  # Down\n",
    "            self.pos[1] -= steps\n",
    "\n",
    "        # Clip the position to stay within the bounds [0, 10]\n",
    "        self.pos = np.clip(self.pos, 0, 10)\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self._get_reward()\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = self._is_done()\n",
    "\n",
    "        return self.pos, reward, done, False, {}\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        if mode == 'console':\n",
    "            print(f\"Current Position: {self.pos}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d7b17-49cb-4c1f-b0f7-75e34ce03f2b",
   "metadata": {},
   "source": [
    "### Train And Test\n",
    "\n",
    "- **Algorithm**  \n",
    "  - The code uses **PPO (Proximal Policy Optimization)**, a reinforcement learning algorithm.  \n",
    "  - PPO optimizes the policy by limiting the step size of updates, ensuring stable learning.  \n",
    "  - It is widely used for continuous control tasks due to its balance between sample efficiency and stability.  \n",
    "\n",
    "- **Process**  \n",
    "  - **Training:**  \n",
    "    - A vectorized environment (`make_vec_env`) is created with 10 parallel instances of the `Golf2DMultiDiscreteBoxEnv`.  \n",
    "    - A PPO model is initialized with the `MlpPolicy` (Multi-Layer Perceptron policy).  \n",
    "    - The model is trained for `300,000` timesteps using the `learn` method.  \n",
    "\n",
    "  - **Testing:**  \n",
    "    - The trained model is tested in a single instance of the `Golf2DMultiDiscreteBoxEnv`.  \n",
    "    - The environment is reset before testing to initialize the observation and info.  \n",
    "    - The model predicts actions step by step for up to 100 iterations or until the environment signals `done`.  \n",
    "    - Observations, actions, rewards, and termination signals are printed during each step.  \n",
    "\n",
    "- **Validation**  \n",
    "  - The test phase evaluates whether the trained policy performs well in the environment.  \n",
    "  - Observing the reward values and termination conditions helps determine the effectiveness of the learned policy.  \n",
    "  - If performance is suboptimal, hyperparameters (e.g., learning rate, network architecture) or the training duration can be adjusted to improve learning.  \n",
    "  - The vectorized training environment ensures efficient exploration and faster convergence by leveraging parallel environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47cd8ca6-2773-4d04-9ef5-cab6dfa92106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: [5 0], action: [2 4], reward: -5, done: False\n",
      "obs: [5 5], action: [1 4], reward: 0, done: True\n"
     ]
    }
   ],
   "source": [
    "def run_golf2d_multidiscrete_box_env():\n",
    "    \n",
    "    def train(NewEnv):\n",
    "        train_env = make_vec_env(lambda: NewEnv(), n_envs=10)\n",
    "        model = PPO('MlpPolicy', env=train_env, verbose=0)\n",
    "        model.learn(total_timesteps=30_0000)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def test(model, env):\n",
    "        obs, info = env.reset()\n",
    "\n",
    "        for i in range(100):\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            print(f'obs: {obs}, action: {action}, reward: {reward}, done: {done}')\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "    model = train(Golf2DMultiDiscreteBoxEnv)\n",
    "    test(model, Golf2DMultiDiscreteBoxEnv())\n",
    "    \n",
    "run_golf2d_multidiscrete_box_env()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
